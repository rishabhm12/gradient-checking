{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y871RZtRhCEH"
   },
   "source": [
    "## `Purpose`: Calculate the gradients for a given computational graph and validate if the gradients are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqePYi-4mFvK"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0huUQ0byiI0I"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "1hSje5CBgcUb",
    "outputId": "66535fa5-f8a2-4792-bc62-7842c93480f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 6)\n",
      "(506, 5) (506,)\n"
     ]
    }
   ],
   "source": [
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(data.shape)\n",
    "X = data[:, :5]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JL-0soQistC"
   },
   "source": [
    "## Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nREnTTJ3i0Vd"
   },
   "source": [
    "<img src='https://i.imgur.com/seSGbNS.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPX_H_4i_HT"
   },
   "source": [
    "\n",
    "*  **If you observe the graph, we are having input features [f1, f2, f3, f4, f5] and 9 weights [w1, w2, w3, w4, w5, w6,    w7, w8, w9]**.<br><br>\n",
    "*  **The final output of this graph is a value L which is computed as (Y-Y')^2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jecTyVRlh-6"
   },
   "source": [
    "<!-- \n",
    "*  <b>Write two functions<br>\n",
    "    *  Forward propagation</b>(Write your code in<font color='blue'> def forward_propagation()</b></font>)<br><br>\n",
    "    For easy debugging, we will break the computational graph into 3 parts.\n",
    "\n",
    "    <font color='green'><b>Part 1</b></font></b>\n",
    "    <img src='https://i.imgur.com/0xUaxy6.png'><br><br>\n",
    "    <font color='green'><b>Part 2</b></font></b><br>\n",
    "    <img src='https://i.imgur.com/J29pAJL.png'><br><br>\n",
    "    <font color='green'><b>Part 3</b></font></b>\n",
    "    <img src='https://i.imgur.com/vMyCsd9.png'>\n",
    "\n",
    "    <pre>\n",
    "    <font color='green'>\n",
    "def forward_propagation(X, y, W):\n",
    "        <font color='grey'>\n",
    "        # X: input data point, note that in this assignment you are having 5-d data points\n",
    "        # y: output varible\n",
    "        # W: weight array, its of length 9, W[0] corresponds to w1 in graph, W[1] corresponds to w2 in graph, <br>         ..., W[8] corresponds to w9 in graph.  \n",
    "        # you have to return the following variables\n",
    "        # exp= part1 (compute the forward propagation until exp and then store the values in exp)\n",
    "        # tanh =part2(compute the forward propagation until tanh and then store the values in tanh)\n",
    "        # sig = part3(compute the forward propagation until sigmoid and then store the values in sig)\n",
    "        # now compute remaining values from computional graph and get y'\n",
    "        # write code to compute the value of L=(y-y')^2\n",
    "        # compute derivative of L  w.r.to Y' and store it in dl\n",
    "        # Create a dictionary to store all the intermediate values\n",
    "        # store L, exp,tanh,sig,dl variables\n",
    "        </font>\n",
    "        return (dictionary, which you might need to use for back propagation)\n",
    "        <font color='grey'>\n",
    "        </font>\n",
    "</font>\n",
    "</pre>\n",
    "    *  <b>Backward propagation</b>(Write your code in<font color='blue'> def backward_propagation()</b></font>)\n",
    "    </b>\n",
    "    <pre>\n",
    "    <font color='green'>\n",
    "    def backward_propagation(L, W,dictionary):\n",
    "        <font color='grey'>\n",
    "        # L: the loss we calculated for the current point\n",
    "        # dictionary: the outputs of the forward_propagation() function\n",
    "        # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n",
    "        # Hint: you can use dict type to store the required variables \n",
    "        # return dW, dW is a dictionary with gradients of all the weights\n",
    "        </font>\n",
    "        return dW\n",
    "        </font>\n",
    "</font>\n",
    "</pre> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrsfpDoidtZ5"
   },
   "source": [
    " we know that the derivative of any function is\n",
    " \n",
    " $$\\lim_{\\epsilon\\to0}\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUcmt0kPd02f"
   },
   "source": [
    "<!-- *  The definition above can be used as a numerical approximation of the derivative. Taking an epsilon small enough, the calculated approximation will have an error in the range of epsilon squared. \n",
    "\n",
    "*  In other words, if epsilon is 0.001, the approximation will be off by 0.00001.\n",
    "\n",
    "Therefore, we can use this to approximate the gradient, and in turn make sure that backpropagation is implemented properly. This forms the basis of <b>gradient checking!</b> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz0mmT_xecfC"
   },
   "source": [
    "<font >\n",
    "lets understand the concept with a simple example:\n",
    "$f(w1,w2,x1,x2)=w_{1}^{2} . x_{1} + w_{2} . x_{2}$ \n",
    "\n",
    "from the above function , lets assume $w_{1}=1$, $w_{2}=2$, $x_{1}=3$, $x_{2}=4$ the gradient of $f$ w.r.t $w_{1}$ is\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "\\frac{df}{dw_{1}} = dw_{1} &=&2.w_{1}.x_{1} \\\\& = &2.1.3\\\\& = &6\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "let calculate the aproximate gradient of $w_{1}$ as mentinoned in the above formula and considering $\\epsilon=0.0001$\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((1+0.0001)^{2} . 3 + 2 . 4) - ((1-0.0001)^{2} . 3 + 2 . 4)}{2\\epsilon} \\\\ & = & \\frac{(1.00020001 . 3 + 2 . 4) - (0.99980001. 3 + 2 . 4)}{2*0.0001} \\\\ & = & \\frac{(11.00060003) - (10.99940003)}{0.0002}\\\\ & = & 5.99999999999\n",
    "\\end{array}\n",
    "\n",
    "Then, we apply the following formula for gradient check: <i>gradient_check</i> = \n",
    "$\\frac{\\left\\Vert\\left (dW-dW^{approx}\\rm\\right) \\right\\Vert_2}{\\left\\Vert\\left (dW\\rm\\right) \\right\\Vert_2+\\left\\Vert\\left (dW^{approx}\\rm\\right) \\right\\Vert_2}$\n",
    "\n",
    "The equation above is basically the Euclidean distance normalized by the sum of the norm of the vectors. We use normalization in case that one of the vectors is very small.\n",
    "As a value for epsilon, we usually opt for 1e-7. Therefore, if gradient check return a value less than 1e-7, then it means that backpropagation was implemented correctly. Otherwise, there is potentially a mistake in our implementation. If the value exceeds 1e-3, then you are sure that the code is not correct.\n",
    "\n",
    "in our example: <i>gradient_check</i> $ = \\frac{(6 - 5.999999999994898)}{(6 + 5.999999999994898)} = 4.2514140356330737e^{-13}$\n",
    "\n",
    "We can mathamatically derive the same thing like this\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((w_{1}+\\epsilon)^{2} . x_{1} + w_{2} . x_{2}) - ((w_{1}-\\epsilon)^{2} . x_{1} + w_{2} . x_{2})}{2\\epsilon} \\\\ & = & \\frac{4. \\epsilon.w_{1}. x_{1}}{2\\epsilon} \\\\ & = &  2.w_{1}.x_{1}\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phG2WDkjkuIL"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfmV3FUgSrBo"
   },
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5_pBivcY30qf"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''In this function, we will compute the sigmoid(z)'''\n",
    "    sigmoid_value = 1/(1+np.exp(-z))\n",
    "    return sigmoid_value\n",
    "    \n",
    "def forward_propagation(x, y, w):\n",
    "    '''In this function, we will compute the forward propagation '''    \n",
    "    data = {}\n",
    "    \n",
    "    # part1\n",
    "    exp_input = (w[0]*x[0] + w[1]*x[1])**2 + w[5]\n",
    "    exp_output = np.exp(exp_input)\n",
    "    data[\"exp\"] = exp_output\n",
    "\n",
    "    # part2\n",
    "    tanh_input = w[6] + exp_output\n",
    "    tanh_ouput = np.tanh(tanh_input)\n",
    "    data[\"tanh\"] = tanh_ouput\n",
    "    \n",
    "    # part3\n",
    "    sigmoid_input = (np.sin(w[2]*x[2]) * (w[3]*x[3] + w[4]*x[4])) + w[7]\n",
    "    sigmoid_output = sigmoid(sigmoid_input)\n",
    "    data[\"sigmoid\"] = sigmoid_output\n",
    "    y_hat = w[8]*sigmoid_output + tanh_ouput\n",
    "    loss = (y - y_hat)**2\n",
    "    data[\"dy_pr\"] = -2*(y - y_hat)\n",
    "    data[\"loss\"] = loss\n",
    "    data[\"y_hat\"] = y_hat\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones(9)*0.9\n",
    "forward_prop_dict = forward_propagation(X[0], y[0], weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp': 12.251152538822089,\n",
       " 'tanh': 0.9999999999924476,\n",
       " 'sigmoid': 0.8328061193327899,\n",
       " 'dy_pr': -0.21864723995882107,\n",
       " 'loss': 0.01195165388540257,\n",
       " 'y_hat': 1.7495255073919584}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_prop_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obOaAqj3Sxvb"
   },
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dS49ykcOGhIv"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(L,W,dict):\n",
    "    '''In this function, we will compute the backward propagation '''\n",
    "    # L: the loss we calculated for the current point\n",
    "    # dictionary: the outputs of the forward_propagation() function\n",
    "    # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n",
    "    # Hint: you can use dict type to store the required variables \n",
    "    # dw1 = # in dw1 compute derivative of L w.r.to w1\n",
    "    # dw2 = # in dw2 compute derivative of L w.r.to w2\n",
    "    # dw3 = # in dw3 compute derivative of L w.r.to w3\n",
    "    # dw4 = # in dw4 compute derivative of L w.r.to w4\n",
    "    # dw5 = # in dw5 compute derivative of L w.r.to w5\n",
    "    # dw6 = # in dw6 compute derivative of L w.r.to w6\n",
    "    # dw7 = # in dw7 compute derivative of L w.r.to w7\n",
    "    # dw8 = # in dw8 compute derivative of L w.r.to w8\n",
    "    # dw9 = # in dw9 compute derivative of L w.r.to w9\n",
    "\n",
    "    # return dW, dW is a dictionary with gradients of all the weights\n",
    "    \n",
    "    derivatives = {}\n",
    "    dw9 = dict[\"dy_pr\"]*dict[\"sigmoid\"]\n",
    "    derivatives[\"dw9\"] = dw9\n",
    "    \n",
    "    dw8 = dict[\"dy_pr\"]*1.0*W[8]*dict[\"sigmoid\"]*(1-dict[\"sigmoid\"])\n",
    "    derivatives[\"dw8\"] = dw8\n",
    "    \n",
    "    dw3 = dw8*(W[3]*L[3] + W[4]*L[4])*np.cos(W[2]*L[2])*L[2]\n",
    "    derivatives[\"dw3\"] = dw3\n",
    "    \n",
    "    dw4 = dw8*np.sin(W[2]*L[2])*L[3]\n",
    "    derivatives[\"dw4\"] = dw4\n",
    "    \n",
    "    dw5 = (dw4*L[4])/L[3]\n",
    "    derivatives[\"dw5\"] = dw5\n",
    "    \n",
    "    \n",
    "    dw7 = dict[\"dy_pr\"]*1.0*(1-(np.tanh(dict[\"exp\"] + W[6])**2))\n",
    "    derivatives[\"dw7\"] = dw7\n",
    "    \n",
    "    dw6 = dw7*dict[\"exp\"]\n",
    "    derivatives[\"dw6\"] = dw6\n",
    "    \n",
    "    dw1 = dw6*(2*(W[0]*L[0] + W[1]*L[1])*L[0])\n",
    "    derivatives[\"dw1\"] = dw1\n",
    "    \n",
    "        \n",
    "    dw2 = dw6*(2*(W[0]*L[0] + W[1]*L[1])*L[1])\n",
    "    derivatives[\"dw2\"] = dw2\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw9': -0.1820907594129311,\n",
       " 'dw8': -0.027400014629898536,\n",
       " 'dw3': -0.0074347511197089865,\n",
       " 'dw4': -0.017661956984688504,\n",
       " 'dw5': -0.003821078192202942,\n",
       " 'dw7': -3.302624206433516e-12,\n",
       " 'dw6': -4.046095293142325e-11,\n",
       " 'dw1': -1.3206046920184315e-10,\n",
       " 'dw2': -1.2306010683558548e-11}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivatives = backward_propagation(X[0],weights, forward_prop_dict)\n",
    "\n",
    "derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rfHHXZwgwo6"
   },
   "source": [
    "## Implementing gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oh8wj1yjGhOi"
   },
   "outputs": [],
   "source": [
    "def gradient_checking(X, y, W, calculated_gradients):\n",
    "    # compute the L value using forward_propagation()\n",
    "    # compute the gradients of W using backword_propagation()\n",
    "    \n",
    "    forward_prop_values = forward_propagation(X[0],y[0],W)\n",
    "    eps = 1e-7\n",
    "    approx_gradients = []\n",
    "    gradient_check_values = []\n",
    "    for i in range(len(W)):\n",
    "        # add a small value to weight wi, and then find the values of L with the updated weights\n",
    "        # subtract a small value to weight wi, and then find the values of L with the updated weights\n",
    "        # compute the approximation gradients of weight wi\n",
    "        \n",
    "        w_cp = W.copy()\n",
    "        w_plus_eps = W[i] + eps\n",
    "        w_minus_eps = W[i] - eps\n",
    "        w_cp[i] = w_plus_eps\n",
    "        y_plus_eps = forward_propagation(X[0],y[0],w_cp)[\"loss\"]\n",
    "        \n",
    "        w_cp[i] = w_minus_eps\n",
    "        y_minus_eps = forward_propagation(X[0],y[0],w_cp)[\"loss\"]\n",
    "        \n",
    "        wi_grad = (y_plus_eps - y_minus_eps)/(2*eps)\n",
    "        approx_gradients.append(wi_grad)\n",
    "        \n",
    "        numerator = np.linalg.norm(calculated_gradients[i] - wi_grad)\n",
    "        denominator = np.linalg.norm(calculated_gradients[i]) + np.linalg.norm(wi_grad)\n",
    "        gradient_check = numerator/denominator\n",
    "        gradient_check_values.append(gradient_check)\n",
    "    \n",
    "    gradient_check_values = np.asarray(gradient_check_values)\n",
    "    \n",
    "    if np.all(gradient_check_values <1e-7):\n",
    "        print(\"All the gradients are correct\")\n",
    "    else:\n",
    "        print(\"Please check your gradients\")\n",
    "        \n",
    "    return gradient_check_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(0,1,9)\n",
    "forward_dict=forward_propagation(X[0],y[0],weights)\n",
    "derivatives=backward_propagation(X[0],weights,forward_dict)\n",
    "\n",
    "calculated_gradients = []\n",
    "for i in range(len(derivatives)):\n",
    "    calculated_gradients.append(derivatives[\"dw{0}\".format((i+1))])\n",
    "\n",
    "calculated_gradients = np.asarray(calculated_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the gradients are correct\n"
     ]
    }
   ],
   "source": [
    "gradient_check = np.asarray(gradient_checking(X, y, weights, calculated_gradients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.34577091e-09, 5.62988740e-10, 4.70959681e-08, 2.26091415e-09,\n",
       "       4.34996591e-08, 1.16407115e-09, 6.00562467e-10, 3.01477517e-11,\n",
       "       2.16702080e-09])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Backpropagation_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
